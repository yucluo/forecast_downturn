{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuchenluo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.deprecated.doc2vec import LabeledSentence\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from string import digits\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "from modAL.density import information_density\n",
    "\n",
    "from scipy.stats import entropy\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"all_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preprocessing -------------------------------\n",
    "punctuation_dictionary = {s:None for s in list(string.punctuation)}\n",
    "\n",
    "punctuation_translator = str.maketrans(punctuation_dictionary)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# (remove punctuation, numbers, lowercase, stop words)\n",
    "def text_cleaner_all(text, punctuation_translator):\n",
    "    text = text.replace('c(\"', '')\n",
    "    text = str(text).translate(punctuation_translator)\n",
    "    text = text.lower()\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    text = text.translate(remove_digits)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    return(text)\n",
    "\n",
    "# (remove punctuation, lowercase, stop words)\n",
    "def text_cleaner_mod(text, punctuation_translator):\n",
    "    text = text.replace('c(\"', '')\n",
    "    text = str(text).translate(punctuation_translator)\n",
    "    text = text.lower()\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    text = ' '.join(filtered_text)\n",
    "    return(text)\n",
    "\n",
    "# (remove punctuation, lowercase)\n",
    "def text_cleaner_min(text, punctuation_translator):\n",
    "    text = text.replace('c(\"', '')\n",
    "    text = str(text).translate(punctuation_translator)\n",
    "    text = text.lower()\n",
    "    return(text)\n",
    "# dat[\"clean_text\"] = dat[\"text\"].apply(lambda x: text_cleaner(x, punctuation_translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find phrases\n",
    "phrases1 = Phrases(map(lambda x: x.split(), dat[\"clean_text\"].tolist())) #bigram\n",
    "phrases2 = Phrases(phrases1[map(lambda x: x.split(), dat[\"clean_text\"].tolist())]) #trigram\n",
    "dat[\"phrased_text\"] = dat[\"clean_text\"].apply(lambda x: \" \".join(phrases2[phrases1[x.split()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>final_code</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>phrased_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7VS1-X791-2R2Y-70SK-00000-00.txt</td>\n",
       "      <td>c(\"May 23--Record demand for area food pantry ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mayrecorddemandareafoodpantryprogramsoutpacing...</td>\n",
       "      <td>may 23record demand for area food_pantry progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4VGG-NH20-TX12-N12V-00000-00.txt</td>\n",
       "      <td>c(\"Jan. 28--STOCKTON -- All but six of the 189...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>janstocktonsixunitsvintagesquareapartmentsrose...</td>\n",
       "      <td>jan 28stockton all but six of the 189 units in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7VTR-NPY0-Y9J0-Y26V-00000-00.txt</td>\n",
       "      <td>c(\"May 31--It's a simple question: What are yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>maysimplequestionchargedturnlightssitcomputert...</td>\n",
       "      <td>may 31its a simple_question what are you being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4VG1-37N0-TX12-N0MF-00000-00.txt</td>\n",
       "      <td>c(\"Jan. 25--This is the best time for buying p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>janbesttimebuyingpropertiesabudhabisaysmdreale...</td>\n",
       "      <td>jan 25this is the best time for buying propert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7WDR-PFY0-Y9J0-Y03G-00000-00.txt</td>\n",
       "      <td>c(\"Aug. 17--GLENS FALLS -- The Glens Falls Far...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>augglensfallsglensfallsfarmersmarketbusyeverla...</td>\n",
       "      <td>aug 17glens falls the glens_falls farmers_mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214539</th>\n",
       "      <td>5P1D-JB81-DYY9-J17B-00000-00.txt</td>\n",
       "      <td>\"Change\" has been President-elect Barack Obama...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>changepresidentelectbarackobamasmantramanysupp...</td>\n",
       "      <td>change has_been presidentelect_barack_obamas m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214540</th>\n",
       "      <td>4V23-HR00-TXJ7-F169-00000-00.txt</td>\n",
       "      <td>As banks race to cover all of their payments b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>banksracecoverpaymentsbasesoneexcitingplayersw...</td>\n",
       "      <td>as banks race to cover all of their payments b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214541</th>\n",
       "      <td>4TKS-1W10-TX2Y-F076-00000-00.txt</td>\n",
       "      <td>Media stocks might be depressed amid the curre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mediastocksmightdepressedamidcurrentfinancialc...</td>\n",
       "      <td>media stocks might be depressed amid the curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214542</th>\n",
       "      <td>7WKK-W2D0-Y9S8-W524-00000-00.txt</td>\n",
       "      <td>Party leaders from both sides have condemned h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>partyleaderssidescondemnedoutburstimmigrationp...</td>\n",
       "      <td>party_leaders from both_sides have condemned h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214543</th>\n",
       "      <td>4TW4-7600-TX7B-K0HM-00000-00.txt</td>\n",
       "      <td>Supporters of a major boost in federal transpo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>supportersmajorboostfederaltransportationspend...</td>\n",
       "      <td>supporters of a major boost in federal transpo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214544 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id  \\\n",
       "0       7VS1-X791-2R2Y-70SK-00000-00.txt   \n",
       "1       4VGG-NH20-TX12-N12V-00000-00.txt   \n",
       "2       7VTR-NPY0-Y9J0-Y26V-00000-00.txt   \n",
       "3       4VG1-37N0-TX12-N0MF-00000-00.txt   \n",
       "4       7WDR-PFY0-Y9J0-Y03G-00000-00.txt   \n",
       "...                                  ...   \n",
       "214539  5P1D-JB81-DYY9-J17B-00000-00.txt   \n",
       "214540  4V23-HR00-TXJ7-F169-00000-00.txt   \n",
       "214541  4TKS-1W10-TX2Y-F076-00000-00.txt   \n",
       "214542  7WKK-W2D0-Y9S8-W524-00000-00.txt   \n",
       "214543  4TW4-7600-TX7B-K0HM-00000-00.txt   \n",
       "\n",
       "                                                     text  final_code  \\\n",
       "0       c(\"May 23--Record demand for area food pantry ...         NaN   \n",
       "1       c(\"Jan. 28--STOCKTON -- All but six of the 189...         NaN   \n",
       "2       c(\"May 31--It's a simple question: What are yo...         NaN   \n",
       "3       c(\"Jan. 25--This is the best time for buying p...         NaN   \n",
       "4       c(\"Aug. 17--GLENS FALLS -- The Glens Falls Far...         NaN   \n",
       "...                                                   ...         ...   \n",
       "214539  \"Change\" has been President-elect Barack Obama...         NaN   \n",
       "214540  As banks race to cover all of their payments b...         NaN   \n",
       "214541  Media stocks might be depressed amid the curre...         NaN   \n",
       "214542  Party leaders from both sides have condemned h...         NaN   \n",
       "214543  Supporters of a major boost in federal transpo...         NaN   \n",
       "\n",
       "                                               clean_text  \\\n",
       "0       mayrecorddemandareafoodpantryprogramsoutpacing...   \n",
       "1       janstocktonsixunitsvintagesquareapartmentsrose...   \n",
       "2       maysimplequestionchargedturnlightssitcomputert...   \n",
       "3       janbesttimebuyingpropertiesabudhabisaysmdreale...   \n",
       "4       augglensfallsglensfallsfarmersmarketbusyeverla...   \n",
       "...                                                   ...   \n",
       "214539  changepresidentelectbarackobamasmantramanysupp...   \n",
       "214540  banksracecoverpaymentsbasesoneexcitingplayersw...   \n",
       "214541  mediastocksmightdepressedamidcurrentfinancialc...   \n",
       "214542  partyleaderssidescondemnedoutburstimmigrationp...   \n",
       "214543  supportersmajorboostfederaltransportationspend...   \n",
       "\n",
       "                                             phrased_text  \n",
       "0       may 23record demand for area food_pantry progr...  \n",
       "1       jan 28stockton all but six of the 189 units in...  \n",
       "2       may 31its a simple_question what are you being...  \n",
       "3       jan 25this is the best time for buying propert...  \n",
       "4       aug 17glens falls the glens_falls farmers_mark...  \n",
       "...                                                   ...  \n",
       "214539  change has_been presidentelect_barack_obamas m...  \n",
       "214540  as banks race to cover all of their payments b...  \n",
       "214541  media stocks might be depressed amid the curre...  \n",
       "214542  party_leaders from both_sides have condemned h...  \n",
       "214543  supporters of a major boost in federal transpo...  \n",
       "\n",
       "[214544 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = list(zip(dat[\"phrased_text\"].tolist(), dat[\"id\"].tolist()))\n",
    "\n",
    "\n",
    "# ## Define an iterator to feed documents and tags to Doc2Vec\n",
    "# class Sentences(object):\n",
    "#     def __init__(self, docs):\n",
    "#         self.docs = docs\n",
    "#     def __iter__(self):\n",
    "#         for doc in self.docs:\n",
    "#             yield TaggedDocument(words=str(doc[0]).split(), tags=[doc[1]])\n",
    "\n",
    "# ## Train and save models\n",
    "# model = Doc2Vec(Sentences(docs), vector_size=100, window=10, min_count=100, negative=10, epochs=10, dm=0, dbow_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214544\n",
      "214544\n",
      "214544\n"
     ]
    }
   ],
   "source": [
    "print(len(model.docvecs))\n",
    "model.save(\"doc2vec_wordvecs.model\")\n",
    "print(len(docs))\n",
    "print(len(dat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deep_recession', 0.8328547477722168),\n",
       " ('economic_slump', 0.823676347732544),\n",
       " ('severe_recession', 0.8173205852508545),\n",
       " ('recessionbut', 0.7941799163818359),\n",
       " ('current_downturn', 0.7936395406723022),\n",
       " ('recessionand', 0.7841596007347107),\n",
       " ('recessionary', 0.7790869474411011),\n",
       " ('prolonged_recession', 0.7782836556434631),\n",
       " ('downturn', 0.7760612964630127),\n",
       " ('recessionit', 0.7659202814102173)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('recession') #check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load labelled data\n",
    "clas_dat1 = pd.read_csv(\"random_sample_CRK_YL-6-7-21.csv\")\n",
    "# keep relavant rows and columns\n",
    "clas_dat1 = clas_dat1[['id', 'text', 'final_code']]\n",
    "clas_dat2 = pd.read_csv(\"random_sample3_CRK.csv\")\n",
    "clas_dat2 = clas_dat2[['id', 'text',\"include\"]]\n",
    "clas_dat2.columns = clas_dat1.columns\n",
    "clas_dat3 = pd.read_csv(\"random_sample2_YL.csv\")\n",
    "clas_dat3 = clas_dat3[['id', 'text',\"match_yl\"]]\n",
    "clas_dat3.columns = clas_dat1.columns\n",
    "clas_dat = clas_dat1.append([clas_dat2, clas_dat3])\n",
    "# ## generate test and training set \n",
    "# clas_dat['test'] = np.random.choice([0, 1], size = len(clas_dat['id']), p = [0.8,0.2])\n",
    "# merge with the total dataframe\n",
    "dat = dat.merge(clas_dat[[\"id\", \"final_code\"]], how = \"left\", on = \"id\")\n",
    "X = np.asarray([model.docvecs[i] for i in dat[dat['id'].isin(clas_dat['id'])].index.tolist()])\n",
    "y = clas_dat['final_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.522593 using {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# split into training and testing \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# # tuning for RFC\n",
    "# parameters = {'n_estimators':[500, 1000, 5000], 'max_depth':[3, 5, 10], \"max_features\": [\"sqrt\", 0.2, 0.5]}\n",
    "# rfc = RandomForestClassifier()\n",
    "# clf = GridSearchCV(rfc, parameters)\n",
    "# clf.fit(X_train, y_train)\n",
    "# print(clf.best_params_) \n",
    "\n",
    "# parameters = {'solver':[\"newton-cg\", \"ibfgs\", \"liblinear\", \"sag\", \"saga\"], 'penalty':[\"none\", \"l1\", \"l2\", \"elasticnet\"], \"C\": [100, 10, 1, 0.1, 0.01]}\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4640819202144156\n",
      "[[42 49]\n",
      " [44 45]]\n",
      "0.48333333333333334\n"
     ]
    }
   ],
   "source": [
    "# test RFC\n",
    "rfc = RandomForestClassifier(n_estimators=5000, max_depth = 3, max_features =\"sqrt\")\n",
    "## Fit the model to the training set\n",
    "rfc.fit(X_train, y_train)\n",
    "#PR AUC\n",
    "preds = rfc.predict_proba(X_test)\n",
    "lr_precision, lr_recall, _= precision_recall_curve(y_test.tolist(),  preds[:,1].tolist())\n",
    "lr_auc = auc(lr_recall, lr_precision)\n",
    "print(lr_auc)\n",
    "confusion = confusion_matrix(y_test, rfc.predict(X_test))\n",
    "print(confusion)\n",
    "print(metrics.accuracy_score(y_test, rfc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  9]\n",
      " [41 21]]\n",
      "Precision: 0.700000\n",
      "Recall: 0.338710\n",
      "[[40 23]\n",
      " [36 21]]\n",
      "Precision: 0.477273\n",
      "Recall: 0.368421\n",
      "[[43 16]\n",
      " [44 17]]\n",
      "Precision: 0.515152\n",
      "Recall: 0.278689\n",
      "[[45 22]\n",
      " [32 21]]\n",
      "Precision: 0.488372\n",
      "Recall: 0.396226\n",
      "[[48  5]\n",
      " [51 16]]\n",
      "Precision: 0.761905\n",
      "Recall: 0.238806\n",
      "[[50 19]\n",
      " [29 22]]\n",
      "Precision: 0.536585\n",
      "Recall: 0.431373\n",
      "[[48 10]\n",
      " [46 16]]\n",
      "Precision: 0.615385\n",
      "Recall: 0.258065\n",
      "[[49 14]\n",
      " [37 20]]\n",
      "Precision: 0.588235\n",
      "Recall: 0.350877\n",
      "[[37 28]\n",
      " [30 25]]\n",
      "Precision: 0.471698\n",
      "Recall: 0.454545\n",
      "[[48 10]\n",
      " [48 14]]\n",
      "Precision: 0.583333\n",
      "Recall: 0.225806\n",
      "Mean Accuracy: 0.5416666666666666\n",
      "Average precision: 0.5737937819249062\n",
      "Average recall: 0.3341517804174145\n"
     ]
    }
   ],
   "source": [
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "    print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.7\n",
      "Average precision: 0.6930884976441068\n",
      "Average recall: 0.6920412161074508\n"
     ]
    }
   ],
   "source": [
    "### Max text cleaning (remove punctuation, numbers, lowercase, stop words) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_all(x, punctuation_translator))\n",
    "# phrases1 = Phrases(map(lambda x: x.split(), clas_dat[\"clean_text\"].tolist())) #bigram\n",
    "# phrases2 = Phrases(phrases1[map(lambda x: x.split(), clas_dat[\"clean_text\"].tolist())]) #trigram\n",
    "# clas_dat[\"clean_text\"] = clas_dat[\"clean_text\"].apply(lambda x: \" \".join(phrases2[phrases1[x.split()]]))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# classifier = RandomForestClassifier(n_estimators=1000,  max_depth = 3, max_features =0.5,random_state=0)\n",
    "# classifier.fit(X_train, y_train) \n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# print(\"Accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "#     print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "#     print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6883333333333334\n",
      "Average precision: 0.6924507709947609\n",
      "Average recall: 0.6900327937486975\n"
     ]
    }
   ],
   "source": [
    "### Moderate text cleaning (remove punctuation, lowercase, stop words) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_mod(x, punctuation_translator))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "#     print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "#     print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6983333333333335\n",
      "Average precision: 0.7212492088350276\n",
      "Average recall: 0.684795987393603\n"
     ]
    }
   ],
   "source": [
    "### Minimum text cleaning (remove punctuation,lowercase) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_min(x, punctuation_translator))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "#     print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "#     print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6872222222222224\n",
      "Average precision: 0.6792422704667211\n",
      "Average recall: 0.702329471843471\n"
     ]
    }
   ],
   "source": [
    "### Minimum text cleaning (remove punctuation,lowercase) + Phrases --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_min(x, punctuation_translator))\n",
    "phrases1 = Phrases(map(lambda x: x.split(), clas_dat[\"clean_text\"].tolist())) #bigram\n",
    "phrases2 = Phrases(phrases1[map(lambda x: x.split(), clas_dat[\"clean_text\"].tolist())]) #trigram\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"clean_text\"].apply(lambda x: \" \".join(phrases2[phrases1[x.split()]]))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "#     print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "#     print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier (BOW+ Tf-Idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.6861111111111111\n",
      "Average precision: 0.6854323686252718\n",
      "Average recall: 0.6815629221432726\n"
     ]
    }
   ],
   "source": [
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = LogisticRegression(random_state=0, C= 1.0, penalty= 'l2', solver= 'liblinear').fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "#     print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "#     print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "#     print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.709167 using {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.678136 (Accuracy: 0.676944) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.677745 (Accuracy: 0.676667) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.678136 (Accuracy: 0.676944) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.698054 (Accuracy: 0.693333) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.698205 (Accuracy: 0.693611) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.697337 (Accuracy: 0.693056) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.726299 (Accuracy: 0.704722) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.726299 (Accuracy: 0.704722) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.722218 (Accuracy: 0.702778) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.744178 (Accuracy: 0.703611) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.744178 (Accuracy: 0.703611) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.743001 (Accuracy: 0.709167) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.813705 (Accuracy: 0.653889) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.813705 (Accuracy: 0.653889) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.753394 (Accuracy: 0.683611) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "#training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.672939 using {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.630882 (0.059695) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.630882 (0.059695) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.630895 (0.058816) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.653695 (0.061467) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.653695 (0.061467) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.652456 (0.061647) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.672939 (0.065714) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.672939 (0.065714) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.672235 (0.066668) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.627845 (0.049367) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.627845 (0.049367) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.634827 (0.054298) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.514613 (0.002171) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.514613 (0.002171) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.529839 (0.010102) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='precision',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "#               'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "#               'kernel': ['rbf']} \n",
    "  \n",
    "# grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 50]\n",
      " [34 51]]\n",
      "Precision: 0.504950\n",
      "Recall: 0.600000\n",
      "[[50 41]\n",
      " [50 39]]\n",
      "Precision: 0.487500\n",
      "Recall: 0.438202\n",
      "[[49 37]\n",
      " [45 49]]\n",
      "Precision: 0.569767\n",
      "Recall: 0.521277\n",
      "[[43 54]\n",
      " [36 47]]\n",
      "Precision: 0.465347\n",
      "Recall: 0.566265\n",
      "[[41 52]\n",
      " [38 49]]\n",
      "Precision: 0.485149\n",
      "Recall: 0.563218\n",
      "[[54 45]\n",
      " [31 50]]\n",
      "Precision: 0.526316\n",
      "Recall: 0.617284\n",
      "[[50 39]\n",
      " [34 57]]\n",
      "Precision: 0.593750\n",
      "Recall: 0.626374\n",
      "[[48 44]\n",
      " [50 38]]\n",
      "Precision: 0.463415\n",
      "Recall: 0.431818\n",
      "[[49 41]\n",
      " [52 38]]\n",
      "Precision: 0.481013\n",
      "Recall: 0.422222\n",
      "[[45 42]\n",
      " [47 46]]\n",
      "Precision: 0.522727\n",
      "Recall: 0.494624\n",
      "Mean Accuracy: 0.5211111111111111\n",
      "Average precision: 0.5099933340990066\n",
      "Average recall: 0.5281283930926547\n"
     ]
    }
   ],
   "source": [
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    classifier = svm.SVC(C = 1, gamma= 0.1, kernel = \"rbf\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "    print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42 42]\n",
      " [48 48]]\n",
      "Precision: 0.533333\n",
      "Recall: 0.500000\n",
      "[[34 44]\n",
      " [47 55]]\n",
      "Precision: 0.555556\n",
      "Recall: 0.539216\n",
      "[[47 36]\n",
      " [40 57]]\n",
      "Precision: 0.612903\n",
      "Recall: 0.587629\n",
      "[[39 47]\n",
      " [41 53]]\n",
      "Precision: 0.530000\n",
      "Recall: 0.563830\n",
      "[[36 54]\n",
      " [40 50]]\n",
      "Precision: 0.480769\n",
      "Recall: 0.555556\n",
      "[[45 37]\n",
      " [41 57]]\n",
      "Precision: 0.606383\n",
      "Recall: 0.581633\n",
      "[[36 46]\n",
      " [45 53]]\n",
      "Precision: 0.535354\n",
      "Recall: 0.540816\n",
      "[[46 34]\n",
      " [50 50]]\n",
      "Precision: 0.595238\n",
      "Recall: 0.500000\n",
      "[[52 33]\n",
      " [47 48]]\n",
      "Precision: 0.592593\n",
      "Recall: 0.505263\n",
      "[[52 34]\n",
      " [53 41]]\n",
      "Precision: 0.546667\n",
      "Recall: 0.436170\n",
      "Mean Accuracy: 0.5227777777777777\n",
      "Average precision: 0.5588795214038866\n",
      "Average recall: 0.531011224529602\n"
     ]
    }
   ],
   "source": [
    "# for robust results, bootstrap the process\n",
    "accuracy_scores= []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    ## load the test set \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    gnb = GaussianNB()    \n",
    "    gnb.fit(X_train, y_train)\n",
    "    y_pred = gnb.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    ## Predict out-of-sample on the test set and compute accuracy\n",
    "    accuracy_scores= accuracy_scores+ [accuracy_score(y_test, y_pred)]\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    precision_scores= precision_scores+ [precision]\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    recall_scores= recall_scores+ [recall]\n",
    "    print('Recall: %f' % recall)\n",
    "\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores)))\n",
    "print(\"Average precision: \" + str(np.mean(precision_scores)))\n",
    "print(\"Average recall: \" + str(np.mean(recall_scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# separate training set by coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.729583 using {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.659427 (Accuracy: 0.677500) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.659427 (Accuracy: 0.677500) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.659144 (Accuracy: 0.677500) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.702105 (Accuracy: 0.709583) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.702105 (Accuracy: 0.709583) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.702114 (Accuracy: 0.710417) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.748412 (Accuracy: 0.729583) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.748412 (Accuracy: 0.729583) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.746675 (Accuracy: 0.729167) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.816216 (Accuracy: 0.695833) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.816216 (Accuracy: 0.695833) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.798491 (Accuracy: 0.701250) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.000000 (Accuracy: 0.527083) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.000000 (Accuracy: 0.527083) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.100000 (Accuracy: 0.529167) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "###### CARLY'S PORTION\n",
    "## load labelled data\n",
    "clas_dat1 = pd.read_csv(\"random_sample_CRK_YL-6-7-21.csv\")\n",
    "# keep relavant rows and columns\n",
    "clas_dat1 = clas_dat1[['id', 'text', 'final_code']]\n",
    "clas_dat2 = pd.read_csv(\"random_sample3_CRK.csv\")\n",
    "clas_dat2 = clas_dat2[['id', 'text',\"include\"]]\n",
    "clas_dat2.columns = clas_dat1.columns\n",
    "# clas_dat3 = pd.read_csv(\"random_sample2_YL.csv\")\n",
    "# clas_dat3 = clas_dat3[['id', 'text',\"match_yl\"]]\n",
    "# clas_dat3.columns = clas_dat1.columns\n",
    "# clas_dat4 = pd.read_csv('random_sample4.csv')\n",
    "clas_dat = clas_dat1.append(clas_dat2)\n",
    "\n",
    "### Max text cleaning (remove punctuation, numbers, lowercase, stop words) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_all(x, punctuation_translator))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "\n",
    "#training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.703117 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.680315 (Accuracy: 0.641519) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.680315 (Accuracy: 0.641519) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.680107 (Accuracy: 0.641519) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.707138 (Accuracy: 0.674071) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.707138 (Accuracy: 0.674071) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.707172 (Accuracy: 0.674435) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.712705 (Accuracy: 0.703117) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.712705 (Accuracy: 0.703117) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.712490 (Accuracy: 0.703117) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.579393 (Accuracy: 0.583890) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.579393 (Accuracy: 0.583890) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.580883 (Accuracy: 0.585688) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.570649 (Accuracy: 0.570649) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.570649 (Accuracy: 0.570649) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.570649 (Accuracy: 0.570649) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "###### YUCHEN'S PORTION\n",
    "## load labelled data\n",
    "clas_dat1 = pd.read_csv(\"random_sample_CRK_YL-6-7-21.csv\")\n",
    "# keep relavant rows and columns\n",
    "clas_dat1 = clas_dat1[['id', 'text', 'final_code']]\n",
    "# clas_dat2 = pd.read_csv(\"random_sample3_CRK.csv\")\n",
    "# clas_dat2 = clas_dat2[['id', 'text',\"include\"]]\n",
    "# clas_dat2.columns = clas_dat1.columns\n",
    "clas_dat3 = pd.read_csv(\"random_sample2_YL.csv\")\n",
    "clas_dat3 = clas_dat3[['id', 'text',\"match_yl\"]]\n",
    "clas_dat3.columns = clas_dat1.columns\n",
    "clas_dat4 = pd.read_csv('random_sample4.csv')\n",
    "clas_dat = clas_dat1.append([clas_dat3, clas_dat4[0:99]])\n",
    "\n",
    "### Max text cleaning (remove punctuation, numbers, lowercase, stop words) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_all(x, punctuation_translator))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "\n",
    "#training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add in new training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.680968 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.681717 (Accuracy: 0.628851) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.681717 (Accuracy: 0.628851) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.681322 (Accuracy: 0.628226) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.686114 (Accuracy: 0.642036) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.686114 (Accuracy: 0.642036) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.685685 (Accuracy: 0.642036) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.684558 (Accuracy: 0.680343) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.684558 (Accuracy: 0.680343) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.684754 (Accuracy: 0.680968) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.586190 (Accuracy: 0.586190) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "## load labelled data\n",
    "clas_dat1 = pd.read_csv(\"random_sample_CRK_YL-6-7-21.csv\")\n",
    "# keep relavant rows and columns\n",
    "clas_dat1 = clas_dat1[['id', 'text', 'final_code']]\n",
    "# clas_dat2 = pd.read_csv(\"random_sample3_CRK.csv\")\n",
    "# clas_dat2 = clas_dat2[['id', 'text',\"include\"]]\n",
    "# clas_dat2.columns = clas_dat1.columns\n",
    "clas_dat3 = pd.read_csv(\"random_sample2_YL.csv\")\n",
    "clas_dat3 = clas_dat3[['id', 'text',\"match_yl\"]]\n",
    "clas_dat3.columns = clas_dat1.columns\n",
    "clas_dat4 = pd.read_csv('random_sample4.csv')\n",
    "clas_dat = clas_dat1.append([clas_dat3, clas_dat4[0:99]])\n",
    "\n",
    "### Max text cleaning (remove punctuation, numbers, lowercase, stop words) --------------------------\n",
    "clas_dat[\"clean_text\"] = clas_dat[\"text\"].apply(lambda x: text_cleaner_all(x, punctuation_translator))\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X = vectorizer.fit_transform(clas_dat[\"clean_text\"]).toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "y = np.array(clas_dat['final_code'])\n",
    "\n",
    "\n",
    "#training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 500) (720,)\n",
      "(716, 500) (716,)\n",
      "Best Accuracy: 0.688877 using {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.656850 (Accuracy: 0.646142) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.657013 (Accuracy: 0.646424) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.657414 (Accuracy: 0.646980) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.685411 (Accuracy: 0.670192) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.685809 (Accuracy: 0.670469) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.686289 (Accuracy: 0.670743) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.706328 (Accuracy: 0.686385) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.706328 (Accuracy: 0.686385) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.704371 (Accuracy: 0.684984) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.679901 (Accuracy: 0.688877) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.679901 (Accuracy: 0.688877) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.676373 (Accuracy: 0.686917) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.515376 (Accuracy: 0.515376) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.515376 (Accuracy: 0.515376) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.517142 (Accuracy: 0.518732) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:647: UserWarning: The covariance matrix associated to your dataset is not full rank\n",
      "  warnings.warn(\"The covariance matrix associated to your dataset \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10299.888678109849934 > -10303.358346807313865). You may want to try with a higher value of support_fraction (current value: 0.852).\n",
      "  RuntimeWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10293.254919369232084 > -10293.868925075987136). You may want to try with a higher value of support_fraction (current value: 0.852).\n",
      "  RuntimeWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10263.449101634279941 > -10293.461392975052149). You may want to try with a higher value of support_fraction (current value: 0.852).\n",
      "  RuntimeWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10325.611687765896932 > -10327.205467276582567). You may want to try with a higher value of support_fraction (current value: 0.852).\n",
      "  RuntimeWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/covariance/_robust_covariance.py:171: RuntimeWarning: Determinant has increased; this should not happen: log(det) > log(previous_det) (-10297.879724535308924 > -10303.275453385576839). You may want to try with a higher value of support_fraction (current value: 0.852).\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(708, 500) (708,)\n",
      "Best Accuracy: 0.685348 using {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.648900 (Accuracy: 0.637320) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.649191 (Accuracy: 0.637320) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.648931 (Accuracy: 0.637324) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.682980 (Accuracy: 0.668157) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.683175 (Accuracy: 0.668439) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.683361 (Accuracy: 0.668720) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.704723 (Accuracy: 0.685348) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.704723 (Accuracy: 0.685348) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.703465 (Accuracy: 0.684503) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.674927 (Accuracy: 0.685344) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.674927 (Accuracy: 0.685344) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.671290 (Accuracy: 0.683380) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.515533 (Accuracy: 0.515533) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.515533 (Accuracy: 0.515533) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.517768 (Accuracy: 0.519771) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "###### Isolation Forest ---------------------------\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "# summarize the shape of the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "iso = IsolationForest(contamination=0.005)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "## -------------------- minimum covariance determinant \n",
    "# identify outliers in the training dataset\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(596, 500) (596,)\n",
      "Best Accuracy: 0.705695 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.702944 (Accuracy: 0.681418) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.702944 (Accuracy: 0.681418) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.702341 (Accuracy: 0.680757) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.721465 (Accuracy: 0.697593) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.721465 (Accuracy: 0.697593) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.720520 (Accuracy: 0.696927) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.726092 (Accuracy: 0.704350) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.726092 (Accuracy: 0.704350) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.726173 (Accuracy: 0.705695) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.617405 (Accuracy: 0.648395) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.617405 (Accuracy: 0.648395) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.620111 (Accuracy: 0.650424) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.533559 (Accuracy: 0.533559) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.533559 (Accuracy: 0.533559) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.533559 (Accuracy: 0.533559) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "## -------------------- LOCAL OUTLIER FACTOR\n",
    "# identify outliers in the training dataset\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor()\n",
    "yhat = lof.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(598, 500) (598,)\n",
      "Best Accuracy: 0.703068 using {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.668877 (Accuracy: 0.652164) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.668877 (Accuracy: 0.652164) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.668877 (Accuracy: 0.652164) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.710149 (Accuracy: 0.688305) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.710149 (Accuracy: 0.688305) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.710149 (Accuracy: 0.688305) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.719766 (Accuracy: 0.702068) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.719766 (Accuracy: 0.702068) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.720326 (Accuracy: 0.703068) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.632597 (Accuracy: 0.666548) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.632597 (Accuracy: 0.666548) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.633583 (Accuracy: 0.667220) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Precision: 0.528418 (Accuracy: 0.528418) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Precision: 0.528418 (Accuracy: 0.528418) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Precision: 0.528418 (Accuracy: 0.528418) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "## -------------------- ONE CLASS SVM\n",
    "# identify outliers in the training dataset\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# identify outliers in the training dataset\n",
    "ee = OneClassSVM(nu=0.1)\n",
    "yhat = ee.fit_predict(X_train)\n",
    "# select all rows that are not outliers\n",
    "mask = yhat != -1\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]\n",
    "# summarize the shape of the updated training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# define grid search\n",
    "scoring = ['accuracy', 'precision']\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 5, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring= ['accuracy', 'precision'],refit = \"accuracy\")\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "precisions = grid_result.cv_results_['mean_test_precision']\n",
    "accuracys =  grid_result.cv_results_['mean_test_accuracy']\n",
    "std_prec = grid_result.cv_results_['std_test_precision']\n",
    "std_acc = grid_result.cv_results_['std_test_accuracy']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for prec, acc, param in zip(precisions, accuracys, params):\n",
    "    print(\"Precision: %f (Accuracy: %f) with: %r\" % (prec, acc, param)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115166 115198  41270  41234 114807  41141   1099 114682 115213  15511]\n"
     ]
    }
   ],
   "source": [
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "from modAL.uncertainty import entropy_sampling\n",
    "from modAL.density import information_density\n",
    "\n",
    "# split into training and testing (can't bootstrap here)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "learner = ActiveLearner(\n",
    "    estimator= RandomForestClassifier(n_estimators=1000, random_state=0),\n",
    "    query_strategy=uncertainty_sampling,\n",
    "    X_training=X_train, y_training=y_train\n",
    ")\n",
    "\n",
    "# generate the unlabelled pool\n",
    "unlabelled = dat[((~dat['id'].isin(clas_dat.id)).tolist())]\n",
    "X_pool = vectorizer.fit_transform(unlabelled[\"phrased_text\"]).toarray()\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X_pool = tfidfconverter.fit_transform(X_pool).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # start the interactive active learning labeling session\n",
    "# accuracy_scores_al = [learner.score(X_test, y_test)]\n",
    "# X_pool_text = unlabelled['clean_text']\n",
    "\n",
    "# for i in range(10):\n",
    "#     query_idx, query_inst = learner.query(X_pool)\n",
    "#     print(X_pool_text[query_idx])\n",
    "#     y_new = np.array([int(input())], dtype=int)\n",
    "#     learner.teach(query_inst, y_new)\n",
    "#     X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "#     accuracy_scores_al.append(learner.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the results \n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Accuracy of the classifier during the active learning')\n",
    "    plt.plot(range(10+1), accuracy_scores_al)\n",
    "    plt.scatter(range(10+1), accuracy_scores_al)\n",
    "    plt.xlabel('number of queries')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelled data\n",
    "clas_dat1 = pd.read_csv(\"random_sample_CRK_YL-6-7-21.csv\")\n",
    "clas_dat2 = pd.read_csv(\"random_sample2.csv\")\n",
    "clas_dat3 = pd.read_csv(\"random_sample3.csv\")\n",
    "\n",
    "labelled_id = clas_dat1['id'].append(clas_dat2['id'])\n",
    "labelled_id = labelled_id.append(clas_dat3['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = dat[~dat['id'].isin(labelled_id)]\n",
    "#sample3 = pool.sample(n = 300)\n",
    "sample4 = pool.sample(n = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample4.to_csv(\"random_sample4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214544"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213644"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample classification (balanced) \n",
    "active learning \n",
    "(not removing numbers) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
